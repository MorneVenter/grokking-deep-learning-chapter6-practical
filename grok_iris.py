# Morne Venter 28634748

import numpy as np
import time
print("=========================================================================")
print("ETA: ~1 minutes.")
print("=========================================================================")
print("Starting program ...")
time.sleep(1)
print("Test firing fans ...")
time.sleep(1)
print("Clearing out CPU cache ...")
time.sleep(1)
print("Inserting plutonium core ...")
time.sleep(1)
print("Starting nuclear reactors ...")
time.sleep(1)
print("Nuclear reactors at 25% capacity ...")
time.sleep(1)
print("Nuclear reactors at 50% capacity ...")
time.sleep(1.2)
print("Nuclear reactors at 70% capacity ...")
time.sleep(1.8)
print("Nuclear reactors at 100% functionality.")
time.sleep(1.5)
print("Starting dark matter drive ...")
time.sleep(1)
print("Dark matter drive at 100% functionality.")
time.sleep(1)
print("Accelerating CPU to warp speed 2 ...")
time.sleep(1)
print("Accelerating CPU to warp speed 6 ...")
time.sleep(1.5)
print("=========================================================================")
print("Maximum warp achieved. Machine learning can begin.")
print("=========================================================================")
time.sleep(2)

np.random.seed(4)
start_time = time.time()

def relu(x):
    return (x > 0) * x

def relu2deriv(output):
    return output>0

#mapping =
#       Iris-setosa      = 1
#       Iris-versicolor  = 2
#       Iris-virginica   = 3

raw_data = np.array([[5.1,3.5,1.4,0.2,1],
                    [4.9,3.0,1.4,0.2,1],
                    [4.7,3.2,1.3,0.2,1],
                    [4.6,3.1,1.5,0.2,1],
                    [5.0,3.6,1.4,0.2,1],
                    [5.4,3.9,1.7,0.4,1],
                    [4.6,3.4,1.4,0.3,1],
                    [5.0,3.4,1.5,0.2,1],
                    [4.4,2.9,1.4,0.2,1],
                    [4.9,3.1,1.5,0.1,1],
                    [5.4,3.7,1.5,0.2,1],
                    [4.8,3.4,1.6,0.2,1],
                    [4.8,3.0,1.4,0.1,1],
                    [4.3,3.0,1.1,0.1,1],
                    [5.8,4.0,1.2,0.2,1],
                    [5.7,4.4,1.5,0.4,1],
                    [5.4,3.9,1.3,0.4,1],
                    [5.1,3.5,1.4,0.3,1],
                    [5.7,3.8,1.7,0.3,1],
                    [5.1,3.8,1.5,0.3,1],
                    [5.4,3.4,1.7,0.2,1],
                    [5.1,3.7,1.5,0.4,1],
                    [4.6,3.6,1.0,0.2,1],
                    [5.1,3.3,1.7,0.5,1],
                    [4.8,3.4,1.9,0.2,1],
                    [5.0,3.0,1.6,0.2,1],
                    [5.0,3.4,1.6,0.4,1],
                    [5.2,3.5,1.5,0.2,1],
                    [5.2,3.4,1.4,0.2,1],
                    [4.7,3.2,1.6,0.2,1],
                    [4.8,3.1,1.6,0.2,1],
                    [5.4,3.4,1.5,0.4,1],
                    [5.2,4.1,1.5,0.1,1],
                    [5.5,4.2,1.4,0.2,1],
                    [4.9,3.1,1.5,0.1,1],
                    [5.0,3.2,1.2,0.2,1],
                    [5.5,3.5,1.3,0.2,1],
                    [4.9,3.1,1.5,0.1,1],
                    [4.4,3.0,1.3,0.2,1],
                    [5.1,3.4,1.5,0.2,1],
                    [5.0,3.5,1.3,0.3,1],
                    [4.5,2.3,1.3,0.3,1],
                    [4.4,3.2,1.3,0.2,1],
                    [5.0,3.5,1.6,0.6,1],
                    [5.1,3.8,1.9,0.4,1],
                    [4.8,3.0,1.4,0.3,1],
                    [5.1,3.8,1.6,0.2,1],
                    [4.6,3.2,1.4,0.2,1],
                    [5.3,3.7,1.5,0.2,1],
                    [5.0,3.3,1.4,0.2,1],
                    [7.0,3.2,4.7,1.4,2],
                    [6.4,3.2,4.5,1.5,2],
                    [6.9,3.1,4.9,1.5,2],
                    [5.5,2.3,4.0,1.3,2],
                    [6.5,2.8,4.6,1.5,2],
                    [5.7,2.8,4.5,1.3,2],
                    [6.3,3.3,4.7,1.6,2],
                    [4.9,2.4,3.3,1.0,2],
                    [6.6,2.9,4.6,1.3,2],
                    [5.2,2.7,3.9,1.4,2],
                    [5.0,2.0,3.5,1.0,2],
                    [5.9,3.0,4.2,1.5,2],
                    [6.0,2.2,4.0,1.0,2],
                    [6.1,2.9,4.7,1.4,2],
                    [5.6,2.9,3.6,1.3,2],
                    [6.7,3.1,4.4,1.4,2],
                    [5.6,3.0,4.5,1.5,2],
                    [5.8,2.7,4.1,1.0,2],
                    [6.2,2.2,4.5,1.5,2],
                    [5.6,2.5,3.9,1.1,2],
                    [5.9,3.2,4.8,1.8,2],
                    [6.1,2.8,4.0,1.3,2],
                    [6.3,2.5,4.9,1.5,2],
                    [6.1,2.8,4.7,1.2,2],
                    [6.4,2.9,4.3,1.3,2],
                    [6.6,3.0,4.4,1.4,2],
                    [6.8,2.8,4.8,1.4,2],
                    [6.7,3.0,5.0,1.7,2],
                    [6.0,2.9,4.5,1.5,2],
                    [5.7,2.6,3.5,1.0,2],
                    [5.5,2.4,3.8,1.1,2],
                    [5.5,2.4,3.7,1.0,2],
                    [5.8,2.7,3.9,1.2,2],
                    [6.0,2.7,5.1,1.6,2],
                    [5.4,3.0,4.5,1.5,2],
                    [6.0,3.4,4.5,1.6,2],
                    [6.7,3.1,4.7,1.5,2],
                    [6.3,2.3,4.4,1.3,2],
                    [5.6,3.0,4.1,1.3,2],
                    [5.5,2.5,4.0,1.3,2],
                    [5.5,2.6,4.4,1.2,2],
                    [6.1,3.0,4.6,1.4,2],
                    [5.8,2.6,4.0,1.2,2],
                    [5.0,2.3,3.3,1.0,2],
                    [5.6,2.7,4.2,1.3,2],
                    [5.7,3.0,4.2,1.2,2],
                    [5.7,2.9,4.2,1.3,2],
                    [6.2,2.9,4.3,1.3,2],
                    [5.1,2.5,3.0,1.1,2],
                    [5.7,2.8,4.1,1.3,2],
                    [6.3,3.3,6.0,2.5,3],
                    [5.8,2.7,5.1,1.9,3],
                    [7.1,3.0,5.9,2.1,3],
                    [6.3,2.9,5.6,1.8,3],
                    [6.5,3.0,5.8,2.2,3],
                    [7.6,3.0,6.6,2.1,3],
                    [4.9,2.5,4.5,1.7,3],
                    [7.3,2.9,6.3,1.8,3],
                    [6.7,2.5,5.8,1.8,3],
                    [7.2,3.6,6.1,2.5,3],
                    [6.5,3.2,5.1,2.0,3],
                    [6.4,2.7,5.3,1.9,3],
                    [6.8,3.0,5.5,2.1,3],
                    [5.7,2.5,5.0,2.0,3],
                    [5.8,2.8,5.1,2.4,3],
                    [6.4,3.2,5.3,2.3,3],
                    [6.5,3.0,5.5,1.8,3],
                    [7.7,3.8,6.7,2.2,3],
                    [7.7,2.6,6.9,2.3,3],
                    [6.0,2.2,5.0,1.5,3],
                    [6.9,3.2,5.7,2.3,3],
                    [5.6,2.8,4.9,2.0,3],
                    [7.7,2.8,6.7,2.0,3],
                    [6.3,2.7,4.9,1.8,3],
                    [6.7,3.3,5.7,2.1,3],
                    [7.2,3.2,6.0,1.8,3],
                    [6.2,2.8,4.8,1.8,3],
                    [6.1,3.0,4.9,1.8,3],
                    [6.4,2.8,5.6,2.1,3],
                    [7.2,3.0,5.8,1.6,3],
                    [7.4,2.8,6.1,1.9,3],
                    [7.9,3.8,6.4,2.0,3],
                    [6.4,2.8,5.6,2.2,3],
                    [6.3,2.8,5.1,1.5,3],
                    [6.1,2.6,5.6,1.4,3],
                    [7.7,3.0,6.1,2.3,3],
                    [6.3,3.4,5.6,2.4,3],
                    [6.4,3.1,5.5,1.8,3],
                    [6.0,3.0,4.8,1.8,3],
                    [6.9,3.1,5.4,2.1,3],
                    [6.7,3.1,5.6,2.4,3],
                    [6.9,3.1,5.1,2.3,3],
                    [5.8,2.7,5.1,1.9,3],
                    [6.8,3.2,5.9,2.3,3],
                    [6.7,3.3,5.7,2.5,3],
                    [6.7,3.0,5.2,2.3,3],
                    [6.3,2.5,5.0,1.9,3],
                    [6.5,3.0,5.2,2.0,3],
                    [6.2,3.4,5.4,2.3,3],
                    [5.9,3.0,5.1,1.8,3]]).astype(np.float32)


input_data = raw_data[:,[0,1,2,3]].astype(np.float32)
result_data = raw_data[:,[4]].astype(np.float32)

alpha = 0.001
hidden_size = 2048
iterations = 11000

weights_0_1 = 2.0*np.random.random((4,hidden_size)).astype(np.float32) - 1.0
weights_1_2 = 2.0*np.random.random((hidden_size,1)).astype(np.float32) - 1.0

layer_2_error = 0

for iteration in range(iterations):
    layer_2_error = 0
    for i in range(len(input_data)):
        layer_0 = input_data[i:i+1]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        layer_2 = np.dot(layer_1,weights_1_2)
        layer_2_error += np.sum((layer_2 - result_data[i:i+1]) ** 2)
        layer_2_delta = (layer_2 - result_data[i:i+1])
        layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)
        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)
    if(iteration % 1000 == 9):
        print("Error:" + str(layer_2_error))

print("=========================================================================")
print("Layer 0 to 1 weights: \n", weights_0_1)
print("Layer 1 to 2 weights: \n", weights_1_2)
print("=========================================================================")
print("Final Error:" + str(layer_2_error))
print("=========================================================================")
print("Time to train: ")
print("--- %s seconds ---" % (time.time() - start_time))
print("=========================================================================\n")
print("========================== TESTING ======================================")
t_input = [5.0,3.4,1.5,0.2]
t_output = [1]
t_layer_1 = relu(np.dot(t_input,weights_0_1))
t_layer_2 = np.dot(t_layer_1,weights_1_2)
print("Using input: ", t_input,". Prediction is:", t_layer_2, ". Correct answer is ", t_output)
t_input = [5.8,2.7,3.9,1.2]
t_output = [2]
t_layer_1 = relu(np.dot(t_input,weights_0_1))
t_layer_2 = np.dot(t_layer_1,weights_1_2)
print("Using input: ", t_input,". Prediction is:", t_layer_2, ". Correct answer is ", t_output)
t_input = [6.2,3.4,5.4,2.3]
t_output = [3]
t_layer_1 = relu(np.dot(t_input,weights_0_1))
t_layer_2 = np.dot(t_layer_1,weights_1_2)
print("Using input: ", t_input,". Prediction is:", t_layer_2, ". Correct answer is ", t_output)
print("=========================================================================")
